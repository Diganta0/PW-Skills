{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose\n",
        "\n",
        "Answer -  Simple Linear Regression (SLR) is a fundamental statistical method and the simplest form of linear regression. It's used to establish a linear relationship between two continuous variables: one independent variable (the predictor, $X$) and one dependent variable (the response, $Y$). The word \"simple\" refers to the fact that it only uses one predictor variable.\n",
        "\n",
        "Purpose of Simple Linear Regression\n",
        "\n",
        "\n",
        "1. Modeling the Relationship: To quantify and describe the strength and nature of the relationship between $X$ and $Y$. It answers questions like: Does higher years of experience lead to a higher salary, and if so, by how much?\n",
        "2. Prediction and Forecasting: To use the established linear relationship to predict the value of the dependent variable ($Y$) for a new, unseen value of the independent variable ($X$). Once the best-fit line is found, you can plug in any $X$ value and get a reasonable estimate for $Y$. For example, after modeling the relationship between temperature ($X$) and ice cream sales ($Y$), you can predict next week's sales if you know the forecast temperature.\n",
        "\n",
        "SLR assumes that the change in the dependent variable is directly proportional to the change in the independent variable, forming a straight line."
      ],
      "metadata": {
        "id": "aKdx55Rp01B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Answer - To ensure that the results and conclusions derived from a Simple Linear Regression model are reliable and statistically valid, four key assumptions must be met. These are often summarized using the acronym LINE:\n",
        "1. L - Linearity: The relationship between the independent variable ($X$) and the dependent variable ($Y$) must be truly linear. This means that the data points should generally follow a straight line when plotted on a scatter plot. If the relationship is curved (non-linear), applying SLR will result in a poor-fitting model and inaccurate predictions.\n",
        "2. I - Independence of Errors (or Residuals): The residuals (the differences between the actual $Y$ values and the predicted $Y$ values) must be independent of each other. This is especially important for time-series data, where the error at one point in time should not be related to the error at the previous point. If errors are dependent (e.g., if one large positive error is usually followed by another positive error), it violates this assumption.\n",
        "3. N - Normality of Errors (or Residuals): The residuals must be approximately normally distributed. When you plot the errors from the model, they should form a symmetric, bell-shaped curve centered around zero. This assumption is crucial for performing statistical significance tests (like t-tests for coefficients).\n",
        "4. E - Equality of Variance (Homoscedasticity): The variance (or spread) of the residuals should be constant across all levels of the independent variable ($X$). When the variance is constant, it is called homoscedasticity. If the spread of the residuals increases or decreases as $X$ increases (a funnel shape), it is called heteroscedasticity, which can lead to inefficient coefficient estimates."
      ],
      "metadata": {
        "id": "J7IHazUa4Xi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "\n",
        "Answer : The simple linear regression model is represented by the following mathematical equation:$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$This equation defines how the dependent variable $Y$ is related to the independent variable $X$.\n",
        "\n",
        "| Term | Role in the Model | Detailed Explanation |\n",
        "|-----------|-----------|-----------|\n",
        "| $Y_i$ | Dependent Variable (Response) | This is the outcome variable we are trying to predict or explain. The subscript $i$ indicates a specific data point (or observation). |\n",
        "| $X_i$ | Independent Variable (Predictor) | This is the variable used to make the prediction. It is the factor that is assumed to influence $Y$. |\n",
        "| $\\beta_0$ | Intercept (or Bias term) | This is a constant term. It represents the predicted average value of $Y$ when the value of $X$ is zero. |\n",
        "| $\\beta_1$ | Slope (or Coefficient for $X$) | This is the most important coefficient. It quantifies the change in $Y$ that is expected to result from a one-unit increase in $X$. It determines the steepness of the line. |\n",
        "| $\\epsilon_i$ | Error Term (Residual) | This term accounts for the difference between the actual observed value $Y_i$ and the value predicted by the line $(\\beta_0 + \\beta_1 X_i)$. It includes the effects of all unobserved factors and inherent randomness. |\n",
        "\n",
        "In simple terms, the equation says: The output value ($Y$) is determined by a starting point ($\\beta_0$), plus a specific rate of influence ($\\beta_1$) from the input ($X$), with some unavoidable random error ($\\epsilon$)."
      ],
      "metadata": {
        "id": "kMCUJEeV4vGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Provide a real-world example where simple linear regression can be applied.\n",
        "\n",
        "Answer : A highly relevant real-world example where Simple Linear Regression (SLR) is widely applied is in Marketing and Sales Analysis.\n",
        "\n",
        "Example: Advertising Spend vs. Sales Revenue\n",
        "- Scenario: A company wants to understand if increasing its advertising budget leads to a proportional increase in sales revenue.\n",
        "- Independent Variable ($X$): The amount of money spent on advertising (e.g., in thousands of dollars).\n",
        "- Dependent Variable ($Y$): The total sales revenue generated (e.g., in thousands of dollars).\n",
        "\n",
        "How SLR is Used:\n",
        "1. Data Collection: The company collects data pairs for several weeks or months (e.g., Week 1: $X=10, Y=120$; Week 2: $X=15, Y=150$, and so on).\n",
        "2. Model Fitting: SLR is used to find the best-fit straight line through this data. The model calculates the optimal slope ($\\beta_1$) and intercept ($\\beta_0$).\n",
        "3. Interpretation:\n",
        "- Intercept ($\\beta_0$): Represents the baseline sales revenue the company expects to generate even if no money is spent on advertising ($X=0$).\n",
        "- Slope ($\\beta_1$): Shows the Return on Investment (ROI) for advertising. If $\\beta_1 = 2.5$, it means for every additional $1,000 spent on advertising, the company can expect an increase of $2,500 in sales revenue.\n",
        "4. Prediction: The company can use the model to predict how much revenue they can expect if they decide to spend a new amount, say $20,000, on advertising next month."
      ],
      "metadata": {
        "id": "n1F1n9Fb7gbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is the method of least squares in linear regression?\n",
        "\n",
        "Answer : The method of least squares (often called Ordinary Least Squares or OLS) is the most common and standard technique used to estimate the unknown coefficients ($\\beta_0$ and $\\beta_1$) of a linear regression model.\n",
        "\n",
        "####The Goal: Minimizing the Error\n",
        "The core challenge in linear regression is drawing the \"best\" straight line through a scatter plot of data points. Since no single line can perfectly pass through every point in real-world data, the method of least squares provides a mathematical definition for \"best\": the line that minimizes the total prediction error.\n",
        "1. Defining the Error (Residuals): For every data point, the error is the vertical distance between the actual data point and the regression line. This error is called the residual ($\\epsilon_i$).\n",
        "2. Squaring the Errors: To prevent positive and negative errors from canceling each other out, and to place a greater penalty on larger errors (outliers), the method squares each residual.\n",
        "3. Minimizing the Sum: The method then calculates the Sum of Squared Errors (SSE) for all data points. The goal of OLS is to find the unique combination of the intercept ($\\beta_0$) and slope ($\\beta_1$) that makes this SSE value as small as possible.\n",
        "\n",
        "By minimizing the sum of the squared residuals, the OLS method ensures that the resulting regression line is the one that lies closest to the vast majority of the data points."
      ],
      "metadata": {
        "id": "1gExQnfDQmHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "Answer : Logistic Regression is a statistical model used for classification problems, rather than regression problems. Despite having \"regression\" in its name, its primary use is to predict the probability that an observation belongs to one of a few discrete classes (most commonly two classes, e.g., \"Pass\" or \"Fail\", \"Yes\" or \"No\").\n",
        "\n",
        "####How Logistic Regression Works\n",
        "Instead of directly predicting a continuous value like linear regression, logistic regression:\n",
        "\n",
        "1. Calculates a linear equation (similar to linear regression).\n",
        "\n",
        "2. Passes the result of this linear equation through a non-linear function called the Sigmoid function (or Logit function).\n",
        "\n",
        "3. The Sigmoid function squashes the output into a value between 0 and 1, which can be interpreted as a probability. For example, a result of 0.8 means an 80% probability of belonging to Class 1.\n",
        "\n",
        "Key Differences from Linear Regression\n",
        "\n",
        "| Feature | Linear Regression (SLR) | Logistic Regression |\n",
        "|-----------|-----------|-----------|\n",
        "| Output Type | Continuous numerical values (e.g., house price, temperature). | Probability of belonging to a class (a value between 0 and 1). |\n",
        "| Core Goal | Predicting a quantity (Estimation). | Predicting a category (Classification). |\n",
        "| Underlying Function | Straight Line Equation ($Y = \\beta_0 + \\beta_1 X$). | Sigmoid Function applied to a linear equation. |\n",
        "| Assumption | Assumes output ($Y$) is normally distributed. | Makes no assumption about the distribution of the output variable. |"
      ],
      "metadata": {
        "id": "ah6lkObkR429"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Name and briefly describe three common evaluation metrics for regression models.\n",
        "\n",
        "Answer : Evaluating a regression model means measuring how close its predictions ($\\hat{Y}$) are to the actual values ($Y$). Here are three key metrics:\n",
        "\n",
        "1. Mean Absolute Error (MAE):\n",
        "- Description: MAE is the average of the absolute differences between the actual values and the model's predictions.\n",
        "- Formula Concept: $\\text{MAE} = \\frac{1}{N} \\sum |Y - \\hat{Y}|$\n",
        "- Advantage: It is highly interpretable because it gives the average error in the original units of the dependent variable ($Y$). It is also less sensitive to outliers compared to MSE.\n",
        "2. Mean Squared Error (MSE):\n",
        "- Description: MSE is the average of the squared differences between the actual values and the predictions.\n",
        "- Formula Concept: $\\text{MSE} = \\frac{1}{N} \\sum (Y - \\hat{Y})^2$\n",
        "- Advantage: By squaring the errors, MSE places a much greater penalty on large errors (outliers). This makes it useful when large prediction mistakes are especially undesirable or costly. The downside is that its units are squared, which makes it less intuitive.\n",
        "3. Root Mean Squared Error (RMSE):\n",
        "- Description: RMSE is simply the square root of the MSE.\n",
        "- Formula Concept: $\\text{RMSE} = \\sqrt{\\text{MSE}}$\n",
        "- Advantage: It has the benefit of MSE's sensitivity to large errors but is much easier to interpret than MSE because it brings the units of the error back to the original units of the dependent variable ($Y$). It's the most widely used metric for regression model evaluation."
      ],
      "metadata": {
        "id": "gMRulw0ATKnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "Answer : The R-squared metric, also known as the Coefficient of Determination, is a crucial goodness-of-fit measure in regression analysis. Its primary purpose is to answer the question: How well does the independent variable(s) explain the variation in the dependent variable?\n",
        "####Key Characteristics and Interpretation\n",
        "1. Proportion of Variance Explained: R-squared is a value between 0 and 1 (or 0% and 100%). It represents the proportion of the total variation in the dependent variable ($Y$) that can be accounted for (or explained) by the changes in the independent variable ($X$) within the model.\n",
        "2. Interpretation Examples:\n",
        "- If R-squared is 0.75 (75%), it means that 75% of the variability observed in $Y$ (e.g., salary fluctuations) is explained by $X$ (e.g., years of experience). The remaining 25% is due to unobserved factors and error.\n",
        "- If R-squared is 0.05 (5%), the model is very poor, and $X$ explains almost none of the changes in $Y$.\n",
        "3. Comparison to the Mean: R-squared essentially compares your fitted model to the most basic model possible, which is simply predicting the mean (average) of the dependent variable for every observation. A good R-squared value means your model performs significantly better than just using the average.\n",
        "\n",
        "A note of caution: While a higher R-squared is generally desirable, simply adding more predictor variables to a model will always artificially increase the R-squared, even if the new variables are irrelevant. This is why the Adjusted R-squared is often used in multiple linear regression, as it corrects for this issue."
      ],
      "metadata": {
        "id": "edH0lX6XUnFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print\n",
        "the slope and intercept. (Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer :This Python code uses the scikit-learn library, the standard for machine learning in Python,\n",
        "to define some sample data, train a simple linear regression model, and then extract the key\n",
        "coefficients (slope and intercept). '''\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([2.5, 5.1, 3.2, 8.5, 1.5, 9.2, 5.5]).reshape(-1, 1)\n",
        "\n",
        "Y = np.array([21, 47, 24, 75, 20, 88, 60])\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X, Y)\n",
        "\n",
        "slope = model.coef_[0]\n",
        "\n",
        "intercept = model.intercept_\n",
        "\n",
        "print(\"--- Model Coefficients ---\")\n",
        "print(f\"Intercept (beta_0): {intercept:.3f}\")\n",
        "print(f\"Slope (beta_1): {slope:.3f}\")\n",
        "\n",
        "hours_to_predict = np.array([[7]])\n",
        "predicted_score = model.predict(hours_to_predict)\n",
        "print(f\"\\nPredicted Score for 7 hours: {predicted_score[0]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukmSWW-sVPaz",
        "outputId": "a01efd10-7412-419d-f73d-7044de7613c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Coefficients ---\n",
            "Intercept (beta_0): 1.189\n",
            "Slope (beta_1): 9.202\n",
            "\n",
            "Predicted Score for 7 hours: 65.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "Answer : Interpreting the coefficients—the Intercept ($\\beta_0$) and the Slope ($\\beta_1$)—is critical for translating the mathematical model back into real-world meaning.\n",
        "1. Interpreting the Intercept ($\\beta_0$).\n",
        "\n",
        "The intercept is the value of the dependent variable ($Y$) when the independent variable ($X$) is exactly zero.\n",
        "- Interpretation: \"When the predictor variable ($X$) is 0, the predicted value of the response variable ($Y$) is equal to $\\beta_0$.\"\n",
        "- Context Check: Sometimes, interpreting the intercept is not meaningful. For example, if $X$ is \"height\" and $Y$ is \"weight,\" an intercept is the predicted weight for someone with zero height, which is physically impossible. In such cases, the intercept is just a necessary component for the best-fit line.\n",
        "\n",
        "2. Interpreting the Slope ($\\beta_1$)\n",
        "\n",
        "The slope is the rate of change; it measures the size of the effect $X$ has on $Y$.\n",
        "- Interpretation: \"For every one-unit increase in the predictor variable ($X$), the predicted value of the response variable ($Y$) is expected to change by $\\beta_1$ (on average), assuming all other factors remain the same\n",
        "- \"Direction and Magnitude:\n",
        "1. If $\\beta_1$ is Positive (e.g., 9.382), $X$ and $Y$ have a positive relationship: as $X$ increases, $Y$ also increases.\n",
        "2. If $\\beta_1$ is Negative (e.g., -5.2), $X$ and $Y$ have a negative relationship: as $X$ increases, $Y$ decreases.\n",
        "3. The absolute value of $\\beta_1$ tells you the magnitude of the impact. A slope of 10 means a much stronger impact than a slope of 1."
      ],
      "metadata": {
        "id": "s-dFnSZ_WkHN"
      }
    }
  ]
}