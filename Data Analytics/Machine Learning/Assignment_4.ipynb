{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans- K-Nearest Neighbors (KNN) is a simple, non-parametric, \"lazy learning\" algorithm.1 It is called \"lazy\" because it doesn't learn a model or mathematical formula during training; instead, it simply memorizes the entire training dataset.\n",
        "\n",
        "How KNN Works:When you want to predict the outcome for a new data point, the algorithm calculates the distance (usually Euclidean) between that new point and every other point in the stored dataset.3 It then identifies the 4$K$ closest points (neighbors).\n",
        "\n",
        "- In Classification: The algorithm uses a majority vote.6 It looks at the classes of the $K$ nearest neighbors. If the majority of neighbors are \"Class A,\" the new point is assigned to \"Class A.\n",
        "- In Regression: The algorithm calculates the average (mean). It looks at the values of the $K$ nearest neighbors and takes their average to predict the value for the new point.\n",
        "\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Ans : The Curse of Dimensionality refers to the various problems that arise when analyzing data in high-dimensional spaces (data with many features/columns).\n",
        "\n",
        "Effect on KNN Performance: KNN is particularly sensitive to this \"curse\" because it relies entirely on calculating distances between points.\n",
        "\n",
        "1. Loss of \"Distance\" Meaning: As the number of dimensions increases, all data points tend to become equidistant (equally far away) from each other. The concept of \"nearest\" neighbor becomes meaningless because the difference in distance between the nearest and farthest point becomes negligible.\n",
        "\n",
        "\n",
        "2. Sparsity: In high dimensions, data becomes very sparse (spread out). To maintain a reliable density of data to find good neighbors, the amount of data needed grows exponentially, which is rarely available.\n",
        "\n",
        "3. Computational Cost: Calculating distances in hundreds or thousands of dimensions is computationally very expensive and slow.\n",
        "\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Ans : Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique. Its goal is to reduce the number of features in a dataset while retaining as much of the original information (variance) as possible.\n",
        "\n",
        "Difference from Feature Selection:\n",
        "\n",
        "- Feature Selection: This involves selecting a subset of the original features and discarding the rest. The features you keep are unchanged. (e.g., Keeping \"Age\" and \"Salary\" but deleting \"Height\").\n",
        "\n",
        "- PCA: This involves transforming the original features into a new set of features called Principal Components. These new components are mathematical combinations of the original features. (e.g., Creating \"Component 1\" which is a mix of Age, Salary, and Height). You lose the original feature names, but you keep their information compressed.\n",
        "\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Ans : Eigenvalues and eigenvectors are mathematical concepts used to calculate the Principal Components (PCs) from the data's covariance matrix.\n",
        "\n",
        "- Eigenvectors (The Direction): These represent the direction of the new axes (Principal Components) where the data has the most spread (variance). The first eigenvector points in the direction of the highest variance.\n",
        "\n",
        "- Eigenvalues (The Magnitude): These numbers represent the amount of variance carried by each eigenvector. A high eigenvalue means that specific Principal Component contains a lot of important information about the data.\n",
        "\n",
        "Importance: They allow us to rank the components. We keep the eigenvectors with the highest eigenvalues (most information) and discard the ones with low eigenvalues (noise), thereby reducing dimensionality.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Ans : KNN and PCA are often used together because PCA solves the major weaknesses of KNN.\n",
        "\n",
        "1. Solving the Curse of Dimensionality: KNN struggles with high-dimensional data (see Q2). PCA reduces the data to a few meaningful components (e.g., from 100 features to 10), making the distance calculations in KNN reliable again.\n",
        "\n",
        "2. Noise Reduction: KNN is sensitive to noisy data (outliers). PCA filters out noise by discarding the components with low variance (low eigenvalues), leaving a cleaner signal for the KNN algorithm to classify.\n",
        "\n",
        "3. Speed: KNN is slow because it calculates distances for every point. Running KNN on PCA-reduced data is significantly faster."
      ],
      "metadata": {
        "id": "kp7ICLm4ldNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load Data\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Train WITHOUT Scaling\n",
        "knn_raw = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_raw.fit(X_train, y_train)\n",
        "acc_raw = accuracy_score(y_test, knn_raw.predict(X_test))\n",
        "\n",
        "# 3. Train WITH Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
        "\n",
        "# 4. Print Results\n",
        "print(\"--- KNN Accuracy Comparison ---\")\n",
        "print(f\"Accuracy without Scaling: {acc_raw:.4f}\")\n",
        "print(f\"Accuracy with Scaling:    {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Seqj6InRgN",
        "outputId": "b62cf2b6-93b7-4825-fe93-7da086e2325a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KNN Accuracy Comparison ---\n",
            "Accuracy without Scaling: 0.7407\n",
            "Accuracy with Scaling:    0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Note: We continue using the 'X_train_scaled' from the previous answer\n",
        "# because PCA requires scaled data.\n",
        "\n",
        "# 1. Initialize PCA (Keep all components for analysis)\n",
        "pca_full = PCA(n_components=None)\n",
        "pca_full.fit(X_train_scaled)\n",
        "\n",
        "# 2. Get Variance Ratios\n",
        "variance_ratios = pca_full.explained_variance_ratio_\n",
        "\n",
        "print(\"--- PCA Explained Variance Ratio ---\")\n",
        "for i, ratio in enumerate(variance_ratios):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nTotal Variance Explained by Top 2: {np.sum(variance_ratios[:2])*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv-Rkoc7nkXm",
        "outputId": "a3798d6c-c9a5-4131-d554-82a58f194448"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PCA Explained Variance Ratio ---\n",
            "Principal Component 1: 0.3620 (36.20%)\n",
            "Principal Component 2: 0.1876 (18.76%)\n",
            "Principal Component 3: 0.1166 (11.66%)\n",
            "Principal Component 4: 0.0758 (7.58%)\n",
            "Principal Component 5: 0.0704 (7.04%)\n",
            "Principal Component 6: 0.0455 (4.55%)\n",
            "Principal Component 7: 0.0358 (3.58%)\n",
            "Principal Component 8: 0.0265 (2.65%)\n",
            "Principal Component 9: 0.0217 (2.17%)\n",
            "Principal Component 10: 0.0196 (1.96%)\n",
            "Principal Component 11: 0.0176 (1.76%)\n",
            "Principal Component 12: 0.0132 (1.32%)\n",
            "Principal Component 13: 0.0076 (0.76%)\n",
            "\n",
            "Total Variance Explained by Top 2: 54.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "# 1. Transform Data using PCA (Keep only Top 2 components)\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_train_pca = pca_2.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_2.transform(X_test_scaled)\n",
        "\n",
        "# 2. Train KNN on PCA Data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "# 3. Comparison\n",
        "print(\"--- Accuracy Comparison: Original vs PCA ---\")\n",
        "print(f\"Original Scaled Data (13 Features): {acc_scaled:.4f}\")\n",
        "print(f\"PCA Reduced Data (2 Features):      {acc_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoAswZAqnpnp",
        "outputId": "fc29c965-bc0b-48c3-96e4-325b1d6eea24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Accuracy Comparison: Original vs PCA ---\n",
            "Original Scaled Data (13 Features): 0.9630\n",
            "PCA Reduced Data (2 Features):      0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (Euclidean, Manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "# 1. Train with Euclidean Distance (p=2)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test_scaled))\n",
        "\n",
        "# 2. Train with Manhattan Distance (p=1)\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test_scaled))\n",
        "\n",
        "# 3. Print Results\n",
        "print(\"--- KNN Distance Metric Comparison ---\")\n",
        "print(f\"Euclidean Distance Accuracy: {acc_euclidean:.4f}\")\n",
        "print(f\"Manhattan Distance Accuracy: {acc_manhattan:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nK3kXFhnw1z",
        "outputId": "8ec266e7-4570-4b7a-8f49-d29228e268c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KNN Distance Metric Comparison ---\n",
            "Euclidean Distance Accuracy: 0.9630\n",
            "Manhattan Distance Accuracy: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit. '''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler  # Added this missing import\n",
        "\n",
        "# 0. Load Data (Ensuring X and y are available)\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 1. Define the Pipeline\n",
        "# Step 1: Scale the data (StandardScaler) - Essential for PCA & KNN\n",
        "# Step 2: Apply PCA (keep 95% variance)\n",
        "# Step 3: Classifier (KNN)\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# 2. Evaluate using Cross-Validation\n",
        "# We use 5-Fold Cross-Validation to simulate a robust evaluation\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "\n",
        "print(\"--- Biomedical Pipeline Strategy ---\")\n",
        "print(f\"Strategy: PCA (95% variance) -> KNN\")\n",
        "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GTxPPq0n0dU",
        "outputId": "71bebace-6783-4e4c-da42-10bb7475e541"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Biomedical Pipeline Strategy ---\n",
            "Strategy: PCA (95% variance) -> KNN\n",
            "Cross-Validation Scores: [0.91666667 0.94444444 0.97222222 1.         0.91428571]\n",
            "Mean Accuracy: 0.9495\n"
          ]
        }
      ]
    }
  ]
}